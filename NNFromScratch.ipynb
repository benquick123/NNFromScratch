{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torchvision.datasets import MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = MNIST(root=\".\", train=True, download=True)\n",
    "train_x = train.data.numpy().astype(np.float32) / 255\n",
    "train_x = train_x.reshape(len(train_x), -1, 1)\n",
    "train_y = train.targets.numpy() # .astype(np.float32)\n",
    "\n",
    "\n",
    "test = MNIST(root=\".\", train=False, download=True)\n",
    "test_x = test.data.numpy().astype(np.float32) / 255\n",
    "test_x = test_x.reshape(len(test_x), -1, 1)\n",
    "test_y = test.targets.numpy() # .astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.activation = None\n",
    "        self.input = []\n",
    "        self.gradient = dict()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def backward(self): \n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "    \n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(Linear, self).__init__()\n",
    "        \n",
    "        self.weights = np.random.randn(out_dim, in_dim) * np.sqrt(2 / in_dim)\n",
    "        self.bias = np.random.rand(out_dim, 1) * np.sqrt(2 / in_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.input.append(x)\n",
    "        self.activation = self.weights @ x + self.bias\n",
    "        return self.activation\n",
    "    \n",
    "    def backward(self, gradient):\n",
    "        self.gradient[\"weights\"] = np.array(self.input[0])\n",
    "        self.gradient[\"bias\"] = np.ones_like(gradient) # is this shape ok?\n",
    "        \n",
    "        gradient_x = self.weights\n",
    "        \n",
    "        self.gradient[\"weights\"] = gradient @ self.gradient[\"weights\"].T\n",
    "        self.gradient[\"bias\"] = self.gradient[\"bias\"] * gradient\n",
    "        gradient_x = gradient_x.T @ gradient\n",
    "        return gradient_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU(Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(ReLU, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.input.append(x)\n",
    "        self.activation = np.where(x < 0, 0, x)\n",
    "        return self.activation\n",
    "    \n",
    "    def backward(self, gradient):\n",
    "        gradient_x = np.where(self.input[0] > 0, 1.0, 0.0)\n",
    "        gradient_x = gradient_x * gradient\n",
    "        return gradient_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax(Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Softmax, self).__init__()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.input.append(x)\n",
    "        self.activation = np.exp(x) / np.sum(np.exp(x))\n",
    "        return self.activation\n",
    "    \n",
    "    def backward(self, gradient):\n",
    "        # compute gradient with respect to self.activation\n",
    "        # we don't use self.input here, because of how Softmax derivative works.\n",
    "        gradient_x = -self.activation @ self.activation.T\n",
    "        gradient_x[range(gradient_x.shape[0]), range(gradient_x.shape[1])] += self.activation.reshape(-1)\n",
    "        \n",
    "        # apply chain rule; multiply with with gradient from the next layer\n",
    "        gradient_x = gradient_x @ gradient\n",
    "        \n",
    "        return gradient_x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLoss(Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(CrossEntropyLoss, self).__init__()\n",
    "    \n",
    "    def forward(self, x, target):\n",
    "        self.input.append(x)\n",
    "        self.input.append(target)\n",
    "        self.activation = -np.log(x.take(target, axis=0))\n",
    "        return self.activation\n",
    "    \n",
    "    def backward(self):\n",
    "        onehot_target = np.zeros_like(self.input[0])\n",
    "        onehot_target[self.input[1]] = 1\n",
    "        gradient = -onehot_target * 1 / self.input[0]\n",
    "        return gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model():\n",
    "    \n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        \n",
    "        self.linear0 = Linear(in_dim, 128)\n",
    "        self.relu0 = ReLU()\n",
    "        self.linear1 = Linear(128, 256)\n",
    "        self.relu1 = ReLU()\n",
    "        self.linear2 = Linear(256, 128)\n",
    "        self.relu2 = ReLU()\n",
    "        self.out = Linear(128, out_dim)\n",
    "        self.softmax = Softmax()\n",
    "        self.loss = CrossEntropyLoss()\n",
    "        \n",
    "        self.modules = [self.linear0, self.relu0, self.linear1, self.relu1, self.linear2, self.relu2, self.out, self.softmax]\n",
    "    \n",
    "    def forward(self, x, target=None):\n",
    "        for module in self.modules:\n",
    "            x = module.forward(x)\n",
    "            if target is None:\n",
    "                module.gradients = dict()\n",
    "                module.input = []\n",
    "                module.activations = None\n",
    "            \n",
    "        y = x\n",
    "        if target is not None:\n",
    "            l = self.loss.forward(y, target)\n",
    "            return y, l\n",
    "        else:\n",
    "            return y\n",
    "            \n",
    "    \n",
    "    def backward(self):\n",
    "        g = self.loss.backward()\n",
    "        for module in self.modules[::-1]:\n",
    "            g = module.backward(g)\n",
    "            \n",
    "    def step(self, lr=0.001):\n",
    "        for module in self.modules:\n",
    "            for param_name, gradient_values in module.gradient.items():\n",
    "                setattr(module, param_name, getattr(module, param_name) - lr * gradient_values)\n",
    "                \n",
    "            module.gradients = dict()\n",
    "            module.input = []\n",
    "            module.activations = None\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784, 1)\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 1\n",
    "N_EPOCHS = 100\n",
    "LR = 0.001\n",
    "\n",
    "print(train_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "EPOCH: 0\n",
      "[0.093 0.069 0.118 0.080 0.067 0.074 0.256 0.062 0.139 0.041 ]\n",
      "Loss: 2.703386862476409\n",
      "-----------------------------------------------------------\n",
      "EPOCH: 1\n",
      "[0.092 0.070 0.111 0.078 0.089 0.067 0.250 0.061 0.143 0.040 ]\n",
      "Loss: 2.4216832176735994\n",
      "-----------------------------------------------------------\n",
      "EPOCH: 2\n",
      "[0.088 0.069 0.099 0.073 0.125 0.059 0.246 0.061 0.145 0.036 ]\n",
      "Loss: 2.082739251514887\n",
      "-----------------------------------------------------------\n",
      "EPOCH: 3\n",
      "[0.081 0.066 0.085 0.067 0.177 0.051 0.239 0.059 0.142 0.033 ]\n",
      "Loss: 1.73274312284929\n",
      "-----------------------------------------------------------\n",
      "EPOCH: 4\n",
      "[0.071 0.065 0.070 0.057 0.266 0.041 0.207 0.058 0.138 0.028 ]\n",
      "Loss: 1.3238017142738778\n",
      "-----------------------------------------------------------\n",
      "EPOCH: 5\n",
      "[0.057 0.058 0.052 0.043 0.389 0.033 0.171 0.054 0.120 0.023 ]\n",
      "Loss: 0.9440253052041981\n",
      "-----------------------------------------------------------\n",
      "EPOCH: 6\n",
      "[0.043 0.049 0.037 0.031 0.539 0.022 0.120 0.043 0.100 0.016 ]\n",
      "Loss: 0.6177467009821855\n",
      "-----------------------------------------------------------\n",
      "EPOCH: 7\n",
      "[0.028 0.034 0.022 0.016 0.703 0.013 0.081 0.028 0.065 0.010 ]\n",
      "Loss: 0.3521851277413404\n",
      "-----------------------------------------------------------\n",
      "EPOCH: 8\n",
      "[0.016 0.022 0.012 0.009 0.825 0.007 0.046 0.017 0.041 0.005 ]\n",
      "Loss: 0.19180310684709315\n",
      "-----------------------------------------------------------\n",
      "EPOCH: 9\n",
      "[0.010 0.014 0.007 0.005 0.892 0.004 0.028 0.010 0.026 0.003 ]\n",
      "Loss: 0.11379420251039048\n",
      "-----------------------------------------------------------\n",
      "EPOCH: 10\n",
      "[0.007 0.010 0.004 0.004 0.927 0.003 0.019 0.007 0.018 0.002 ]\n",
      "Loss: 0.0763085669544635\n",
      "-----------------------------------------------------------\n",
      "EPOCH: 11\n",
      "[0.005 0.007 0.003 0.003 0.945 0.002 0.014 0.005 0.013 0.001 ]\n",
      "Loss: 0.05621162711669438\n",
      "-----------------------------------------------------------\n",
      "EPOCH: 12\n",
      "[0.004 0.006 0.002 0.002 0.957 0.002 0.011 0.004 0.011 0.001 ]\n",
      "Loss: 0.04420106528190399\n",
      "-----------------------------------------------------------\n",
      "EPOCH: 13\n",
      "[0.004 0.005 0.002 0.002 0.965 0.001 0.009 0.003 0.009 0.001 ]\n",
      "Loss: 0.03609096812586335\n",
      "-----------------------------------------------------------\n",
      "EPOCH: 14\n",
      "[0.003 0.004 0.002 0.001 0.970 0.001 0.008 0.003 0.007 0.001 ]\n",
      "Loss: 0.030373153912446403\n",
      "-----------------------------------------------------------\n",
      "EPOCH: 15\n",
      "[0.003 0.004 0.001 0.001 0.974 0.001 0.007 0.002 0.007 0.001 ]\n",
      "Loss: 0.026272763170307935\n",
      "-----------------------------------------------------------\n",
      "EPOCH: 16\n",
      "[0.002 0.003 0.001 0.001 0.977 0.001 0.006 0.002 0.006 0.001 ]\n",
      "Loss: 0.023081968143752823\n",
      "-----------------------------------------------------------\n",
      "EPOCH: 17\n",
      "[0.002 0.003 0.001 0.001 0.980 0.001 0.005 0.002 0.005 0.001 ]\n",
      "Loss: 0.020558441855435244\n",
      "-----------------------------------------------------------\n",
      "EPOCH: 18\n",
      "[0.002 0.003 0.001 0.001 0.982 0.001 0.005 0.002 0.005 0.000 ]\n",
      "Loss: 0.018469312881308347\n",
      "-----------------------------------------------------------\n",
      "EPOCH: 19\n",
      "[0.002 0.002 0.001 0.001 0.983 0.001 0.004 0.001 0.004 0.000 ]\n",
      "Loss: 0.01674307003419059\n",
      "-----------------------------------------------------------\n",
      "EPOCH: 20\n",
      "[0.002 0.002 0.001 0.001 0.985 0.001 0.004 0.001 0.004 0.000 ]\n",
      "Loss: 0.01531204701702513\n",
      "-----------------------------------------------------------\n",
      "EPOCH: 21\n",
      "[0.001 0.002 0.001 0.001 0.986 0.000 0.004 0.001 0.004 0.000 ]\n",
      "Loss: 0.014092535565260883\n",
      "-----------------------------------------------------------\n",
      "EPOCH: 22\n",
      "[0.001 0.002 0.001 0.001 0.987 0.000 0.003 0.001 0.003 0.000 ]\n",
      "Loss: 0.013024606879199914\n",
      "-----------------------------------------------------------\n",
      "EPOCH: 23\n",
      "[0.001 0.002 0.001 0.001 0.988 0.000 0.003 0.001 0.003 0.000 ]\n",
      "Loss: 0.012102529001643337\n",
      "-----------------------------------------------------------\n",
      "EPOCH: 24\n",
      "[0.001 0.002 0.001 0.001 0.989 0.000 0.003 0.001 0.003 0.000 ]\n",
      "Loss: 0.011309654308072908\n",
      "-----------------------------------------------------------\n",
      "EPOCH: 25\n",
      "[0.001 0.001 0.001 0.000 0.989 0.000 0.003 0.001 0.003 0.000 ]\n",
      "Loss: 0.010593857833066326\n",
      "-----------------------------------------------------------\n",
      "EPOCH: 26\n",
      "[0.001 0.001 0.000 0.000 0.990 0.000 0.003 0.001 0.003 0.000 ]\n",
      "Loss: 0.009957010538843954\n",
      "-----------------------------------------------------------\n",
      "EPOCH: 27\n",
      "[0.001 0.001 0.000 0.000 0.991 0.000 0.002 0.001 0.002 0.000 ]\n",
      "Loss: 0.009390533716985252\n",
      "-----------------------------------------------------------\n",
      "EPOCH: 28\n",
      "[0.001 0.001 0.000 0.000 0.991 0.000 0.002 0.001 0.002 0.000 ]\n",
      "Loss: 0.008877333722861\n",
      "-----------------------------------------------------------\n",
      "EPOCH: 29\n",
      "[0.001 0.001 0.000 0.000 0.992 0.000 0.002 0.001 0.002 0.000 ]\n",
      "Loss: 0.008420100456933834\n",
      "-----------------------------------------------------------\n",
      "EPOCH: 30\n",
      "[0.001 0.001 0.000 0.000 0.992 0.000 0.002 0.001 0.002 0.000 ]\n",
      "Loss: 0.007999466185363797\n",
      "-----------------------------------------------------------\n",
      "EPOCH: 31\n",
      "[0.001 0.001 0.000 0.000 0.992 0.000 0.002 0.001 0.002 0.000 ]\n",
      "Loss: 0.007613084715164236\n",
      "-----------------------------------------------------------\n",
      "EPOCH: 32\n",
      "[0.001 0.001 0.000 0.000 0.993 0.000 0.002 0.001 0.002 0.000 ]\n",
      "Loss: 0.007268944340522446\n",
      "-----------------------------------------------------------\n",
      "EPOCH: 33\n",
      "[0.001 0.001 0.000 0.000 0.993 0.000 0.002 0.001 0.002 0.000 ]\n",
      "Loss: 0.0069547246750553595\n",
      "-----------------------------------------------------------\n",
      "EPOCH: 34\n",
      "[0.001 0.001 0.000 0.000 0.993 0.000 0.002 0.001 0.002 0.000 ]\n",
      "Loss: 0.006655062374942855\n",
      "-----------------------------------------------------------\n",
      "EPOCH: 35\n",
      "[0.001 0.001 0.000 0.000 0.994 0.000 0.002 0.001 0.002 0.000 ]\n",
      "Loss: 0.006383203501275516\n",
      "-----------------------------------------------------------\n",
      "EPOCH: 36\n",
      "[0.001 0.001 0.000 0.000 0.994 0.000 0.002 0.001 0.002 0.000 ]\n",
      "Loss: 0.006130190046321487\n",
      "-----------------------------------------------------------\n",
      "EPOCH: 37\n",
      "[0.001 0.001 0.000 0.000 0.994 0.000 0.002 0.000 0.002 0.000 ]\n",
      "Loss: 0.005896443907139441\n",
      "-----------------------------------------------------------\n",
      "EPOCH: 38\n",
      "[0.001 0.001 0.000 0.000 0.994 0.000 0.001 0.000 0.002 0.000 ]\n",
      "Loss: 0.005677705322329425\n",
      "-----------------------------------------------------------\n",
      "EPOCH: 39\n",
      "[0.001 0.001 0.000 0.000 0.995 0.000 0.001 0.000 0.001 0.000 ]\n",
      "Loss: 0.005473782294163705\n",
      "-----------------------------------------------------------\n",
      "EPOCH: 40\n",
      "[0.001 0.001 0.000 0.000 0.995 0.000 0.001 0.000 0.001 0.000 ]\n",
      "Loss: 0.005281180001137691\n",
      "-----------------------------------------------------------\n",
      "EPOCH: 41\n",
      "[0.001 0.001 0.000 0.000 0.995 0.000 0.001 0.000 0.001 0.000 ]\n",
      "Loss: 0.005102334156265013\n",
      "-----------------------------------------------------------\n",
      "EPOCH: 42\n",
      "[0.001 0.001 0.000 0.000 0.995 0.000 0.001 0.000 0.001 0.000 ]\n",
      "Loss: 0.004935048619045929\n",
      "-----------------------------------------------------------\n",
      "EPOCH: 43\n",
      "[0.000 0.001 0.000 0.000 0.995 0.000 0.001 0.000 0.001 0.000 ]\n",
      "Loss: 0.00477638196589344\n",
      "-----------------------------------------------------------\n",
      "EPOCH: 44\n",
      "[0.000 0.001 0.000 0.000 0.995 0.000 0.001 0.000 0.001 0.000 ]\n",
      "Loss: 0.004626969688336985\n",
      "-----------------------------------------------------------\n",
      "EPOCH: 45\n",
      "[0.000 0.001 0.000 0.000 0.996 0.000 0.001 0.000 0.001 0.000 ]\n",
      "Loss: 0.004488100578521298\n",
      "-----------------------------------------------------------\n",
      "EPOCH: 46\n",
      "[0.000 0.001 0.000 0.000 0.996 0.000 0.001 0.000 0.001 0.000 ]\n",
      "Loss: 0.004354483705383601\n",
      "-----------------------------------------------------------\n",
      "EPOCH: 47\n",
      "[0.000 0.001 0.000 0.000 0.996 0.000 0.001 0.000 0.001 0.000 ]\n",
      "Loss: 0.004228812839329724\n",
      "-----------------------------------------------------------\n",
      "EPOCH: 48\n",
      "[0.000 0.001 0.000 0.000 0.996 0.000 0.001 0.000 0.001 0.000 ]\n",
      "Loss: 0.004108837783421779\n",
      "-----------------------------------------------------------\n",
      "EPOCH: 49\n",
      "[0.000 0.001 0.000 0.000 0.996 0.000 0.001 0.000 0.001 0.000 ]\n",
      "Loss: 0.003995377354650899\n",
      "-----------------------------------------------------------\n",
      "EPOCH: 50\n",
      "[0.000 0.001 0.000 0.000 0.996 0.000 0.001 0.000 0.001 0.000 ]\n",
      "Loss: 0.0038892567912452007\n",
      "-----------------------------------------------------------\n",
      "EPOCH: 51\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.000 0.001 0.000 0.000 0.996 0.000 0.001 0.000 0.001 0.000 ]\n",
      "Loss: 0.003786430998724024\n",
      "-----------------------------------------------------------\n",
      "EPOCH: 52\n",
      "[0.000 0.001 0.000 0.000 0.996 0.000 0.001 0.000 0.001 0.000 ]\n",
      "Loss: 0.003688513453771797\n",
      "-----------------------------------------------------------\n",
      "EPOCH: 53\n",
      "[0.000 0.001 0.000 0.000 0.996 0.000 0.001 0.000 0.001 0.000 ]\n",
      "Loss: 0.0035945599348278664\n",
      "-----------------------------------------------------------\n",
      "EPOCH: 54\n",
      "[0.000 0.000 0.000 0.000 0.996 0.000 0.001 0.000 0.001 0.000 ]\n",
      "Loss: 0.003506663473758732\n",
      "-----------------------------------------------------------\n",
      "EPOCH: 55\n",
      "[0.000 0.000 0.000 0.000 0.997 0.000 0.001 0.000 0.001 0.000 ]\n",
      "Loss: 0.0034226937477174354\n",
      "-----------------------------------------------------------\n",
      "EPOCH: 56\n",
      "[0.000 0.000 0.000 0.000 0.997 0.000 0.001 0.000 0.001 0.000 ]\n",
      "Loss: 0.003340585273289531\n",
      "-----------------------------------------------------------\n",
      "EPOCH: 57\n",
      "[0.000 0.000 0.000 0.000 0.997 0.000 0.001 0.000 0.001 0.000 ]\n",
      "Loss: 0.0032633054916861495\n",
      "-----------------------------------------------------------\n",
      "EPOCH: 58\n",
      "[0.000 0.000 0.000 0.000 0.997 0.000 0.001 0.000 0.001 0.000 ]\n",
      "Loss: 0.003189267297662285\n",
      "-----------------------------------------------------------\n",
      "EPOCH: 59\n",
      "[0.000 0.000 0.000 0.000 0.997 0.000 0.001 0.000 0.001 0.000 ]\n",
      "Loss: 0.0031177377114431693\n",
      "-----------------------------------------------------------\n",
      "EPOCH: 60\n",
      "[0.000 0.000 0.000 0.000 0.997 0.000 0.001 0.000 0.001 0.000 ]\n",
      "Loss: 0.0030502591117801134\n",
      "-----------------------------------------------------------\n",
      "EPOCH: 61\n",
      "[0.000 0.000 0.000 0.000 0.997 0.000 0.001 0.000 0.001 0.000 ]\n",
      "Loss: 0.0029842077774229164\n",
      "-----------------------------------------------------------\n",
      "EPOCH: 62\n",
      "[0.000 0.000 0.000 0.000 0.997 0.000 0.001 0.000 0.001 0.000 ]\n",
      "Loss: 0.0029208793453666605\n",
      "-----------------------------------------------------------\n",
      "EPOCH: 63\n",
      "[0.000 0.000 0.000 0.000 0.997 0.000 0.001 0.000 0.001 0.000 ]\n",
      "Loss: 0.0028608547286000314\n",
      "-----------------------------------------------------------\n",
      "EPOCH: 64\n",
      "[0.000 0.000 0.000 0.000 0.997 0.000 0.001 0.000 0.001 0.000 ]\n",
      "Loss: 0.002802742172516653\n",
      "-----------------------------------------------------------\n",
      "EPOCH: 65\n",
      "[0.000 0.000 0.000 0.000 0.997 0.000 0.001 0.000 0.001 0.000 ]\n",
      "Loss: 0.0027464251863666455\n",
      "-----------------------------------------------------------\n",
      "EPOCH: 66\n",
      "[0.000 0.000 0.000 0.000 0.997 0.000 0.001 0.000 0.001 0.000 ]\n",
      "Loss: 0.002691763429172981\n",
      "-----------------------------------------------------------\n",
      "EPOCH: 67\n",
      "[0.000 0.000 0.000 0.000 0.997 0.000 0.001 0.000 0.001 0.000 ]\n",
      "Loss: 0.0026404944468813606\n",
      "-----------------------------------------------------------\n",
      "EPOCH: 68\n",
      "[0.000 0.000 0.000 0.000 0.997 0.000 0.001 0.000 0.001 0.000 ]\n",
      "Loss: 0.002589644935061861\n",
      "-----------------------------------------------------------\n",
      "EPOCH: 69\n",
      "[0.000 0.000 0.000 0.000 0.997 0.000 0.001 0.000 0.001 0.000 ]\n",
      "Loss: 0.0025412759339997417\n",
      "-----------------------------------------------------------\n",
      "EPOCH: 70\n",
      "[0.000 0.000 0.000 0.000 0.998 0.000 0.001 0.000 0.001 0.000 ]\n",
      "Loss: 0.0024943486207501097\n",
      "-----------------------------------------------------------\n",
      "EPOCH: 71\n",
      "[0.000 0.000 0.000 0.000 0.998 0.000 0.001 0.000 0.001 0.000 ]\n",
      "Loss: 0.002448729730270002\n",
      "-----------------------------------------------------------\n",
      "EPOCH: 72\n",
      "[0.000 0.000 0.000 0.000 0.998 0.000 0.001 0.000 0.001 0.000 ]\n",
      "Loss: 0.002406327485126197\n",
      "-----------------------------------------------------------\n",
      "EPOCH: 73\n",
      "[0.000 0.000 0.000 0.000 0.998 0.000 0.001 0.000 0.001 0.000 ]\n",
      "Loss: 0.002363442503934654\n",
      "-----------------------------------------------------------\n",
      "EPOCH: 74\n",
      "[0.000 0.000 0.000 0.000 0.998 0.000 0.001 0.000 0.001 0.000 ]\n",
      "Loss: 0.0023223638093199904\n",
      "-----------------------------------------------------------\n",
      "EPOCH: 75\n",
      "[0.000 0.000 0.000 0.000 0.998 0.000 0.001 0.000 0.001 0.000 ]\n",
      "Loss: 0.0022829155930408672\n",
      "-----------------------------------------------------------\n",
      "EPOCH: 76\n",
      "[0.000 0.000 0.000 0.000 0.998 0.000 0.001 0.000 0.001 0.000 ]\n",
      "Loss: 0.0022445105392412423\n",
      "-----------------------------------------------------------\n",
      "EPOCH: 77\n",
      "[0.000 0.000 0.000 0.000 0.998 0.000 0.001 0.000 0.001 0.000 ]\n",
      "Loss: 0.002207400992304587\n",
      "-----------------------------------------------------------\n",
      "EPOCH: 78\n",
      "[0.000 0.000 0.000 0.000 0.998 0.000 0.001 0.000 0.001 0.000 ]\n",
      "Loss: 0.0021714550643793656\n",
      "-----------------------------------------------------------\n",
      "EPOCH: 79\n",
      "[0.000 0.000 0.000 0.000 0.998 0.000 0.001 0.000 0.001 0.000 ]\n",
      "Loss: 0.0021364627837294763\n",
      "-----------------------------------------------------------\n",
      "EPOCH: 80\n",
      "[0.000 0.000 0.000 0.000 0.998 0.000 0.001 0.000 0.001 0.000 ]\n",
      "Loss: 0.002102671870546465\n",
      "-----------------------------------------------------------\n",
      "EPOCH: 81\n",
      "[0.000 0.000 0.000 0.000 0.998 0.000 0.001 0.000 0.001 0.000 ]\n",
      "Loss: 0.0020695015987478517\n",
      "-----------------------------------------------------------\n",
      "EPOCH: 82\n",
      "[0.000 0.000 0.000 0.000 0.998 0.000 0.001 0.000 0.001 0.000 ]\n",
      "Loss: 0.0020374407685042574\n",
      "-----------------------------------------------------------\n",
      "EPOCH: 83\n",
      "[0.000 0.000 0.000 0.000 0.998 0.000 0.001 0.000 0.001 0.000 ]\n",
      "Loss: 0.002006942334759791\n",
      "-----------------------------------------------------------\n",
      "EPOCH: 84\n",
      "[0.000 0.000 0.000 0.000 0.998 0.000 0.001 0.000 0.001 0.000 ]\n",
      "Loss: 0.0019760900857157043\n",
      "-----------------------------------------------------------\n",
      "EPOCH: 85\n",
      "[0.000 0.000 0.000 0.000 0.998 0.000 0.001 0.000 0.001 0.000 ]\n",
      "Loss: 0.0019467158478777098\n",
      "-----------------------------------------------------------\n",
      "EPOCH: 86\n",
      "[0.000 0.000 0.000 0.000 0.998 0.000 0.000 0.000 0.001 0.000 ]\n",
      "Loss: 0.0019183325101526998\n",
      "-----------------------------------------------------------\n",
      "EPOCH: 87\n",
      "[0.000 0.000 0.000 0.000 0.998 0.000 0.000 0.000 0.001 0.000 ]\n",
      "Loss: 0.0018906190001977288\n",
      "-----------------------------------------------------------\n",
      "EPOCH: 88\n",
      "[0.000 0.000 0.000 0.000 0.998 0.000 0.000 0.000 0.001 0.000 ]\n",
      "Loss: 0.0018631974902769102\n",
      "-----------------------------------------------------------\n",
      "EPOCH: 89\n",
      "[0.000 0.000 0.000 0.000 0.998 0.000 0.000 0.000 0.001 0.000 ]\n",
      "Loss: 0.0018372032004792367\n",
      "-----------------------------------------------------------\n",
      "EPOCH: 90\n",
      "[0.000 0.000 0.000 0.000 0.998 0.000 0.000 0.000 0.001 0.000 ]\n",
      "Loss: 0.0018115866537330664\n",
      "-----------------------------------------------------------\n",
      "EPOCH: 91\n",
      "[0.000 0.000 0.000 0.000 0.998 0.000 0.000 0.000 0.001 0.000 ]\n",
      "Loss: 0.0017867362686368325\n",
      "-----------------------------------------------------------\n",
      "EPOCH: 92\n",
      "[0.000 0.000 0.000 0.000 0.998 0.000 0.000 0.000 0.001 0.000 ]\n",
      "Loss: 0.0017619225816284392\n",
      "-----------------------------------------------------------\n",
      "EPOCH: 93\n",
      "[0.000 0.000 0.000 0.000 0.998 0.000 0.000 0.000 0.000 0.000 ]\n",
      "Loss: 0.0017381825216853453\n",
      "-----------------------------------------------------------\n",
      "EPOCH: 94\n",
      "[0.000 0.000 0.000 0.000 0.998 0.000 0.000 0.000 0.000 0.000 ]\n",
      "Loss: 0.0017151673577725192\n",
      "-----------------------------------------------------------\n",
      "EPOCH: 95\n",
      "[0.000 0.000 0.000 0.000 0.998 0.000 0.000 0.000 0.000 0.000 ]\n",
      "Loss: 0.0016924555023410941\n",
      "-----------------------------------------------------------\n",
      "EPOCH: 96\n",
      "[0.000 0.000 0.000 0.000 0.998 0.000 0.000 0.000 0.000 0.000 ]\n",
      "Loss: 0.0016704234017407965\n",
      "-----------------------------------------------------------\n",
      "EPOCH: 97\n",
      "[0.000 0.000 0.000 0.000 0.998 0.000 0.000 0.000 0.000 0.000 ]\n",
      "Loss: 0.0016489971001656624\n",
      "-----------------------------------------------------------\n",
      "EPOCH: 98\n",
      "[0.000 0.000 0.000 0.000 0.998 0.000 0.000 0.000 0.000 0.000 ]\n",
      "Loss: 0.0016276498689802388\n",
      "-----------------------------------------------------------\n",
      "EPOCH: 99\n",
      "[0.000 0.000 0.000 0.000 0.998 0.000 0.000 0.000 0.000 0.000 ]\n",
      "Loss: 0.0016070611261356385\n",
      "-----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "model = Model(train_x.shape[1], len(np.unique(train_y)))\n",
    "\n",
    "sample_index = np.random.randint(len(train_x))\n",
    "sample_x = train_x[sample_index]\n",
    "sample_y = train_y[sample_index]\n",
    "print(sample_y)\n",
    "\n",
    "for i in range(N_EPOCHS):\n",
    "    print(\"EPOCH:\", i)\n",
    "    y, l = model.forward(sample_x, sample_y)\n",
    "    print(\"[\", *[\"%.3f \" % p for p in y.reshape(-1)], \"]\", sep=\"\")\n",
    "    \n",
    "    model.backward()\n",
    "    model.step(lr=LR)\n",
    "    print(\"Loss:\", l[0])\n",
    "    print(\"-----------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784, 1)\n",
      "EPOCH: 0\n",
      "Loss: 16.11518230369279\n",
      "Test accuracy: 0.0892\n",
      "EPOCH: 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-ac832c3aca04>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;31m# print(\"-----------------------------------------------------------\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mLR\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-ff19226bd43b>\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m             \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-89ecebd352fa>\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient)\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mgradient_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"weights\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgradient\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"weights\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"bias\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"bias\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mgradient_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgradient_x\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = Model(train_x.shape[1], len(np.unique(train_y)))\n",
    "\n",
    "for i in range(N_EPOCHS):\n",
    "    print(\"EPOCH:\", i)\n",
    "    for batch_idx in range(0, len(train_x), BATCH_SIZE):\n",
    "        y, l = model.forward(train_x[batch_idx], train_y[batch_idx])\n",
    "        # print(\"[\", *[\"%.3f \" % p for p in y.reshape(-1)], \"]\", sep=\"\")\n",
    "        # print(\"-----------------------------------------------------------\")\n",
    "\n",
    "        model.backward()\n",
    "        model.step(lr=LR)\n",
    "        \n",
    "    print(\"Loss:\", l[0])\n",
    "    \n",
    "    test_pred = model.forward(test_x).reshape(-1, 10)\n",
    "    print(\"Test accuracy:\", np.sum(np.argmax(model.forward(test_x).reshape(-1, 10), axis=1) == test_y) / len(test_x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit",
   "language": "python",
   "name": "python37464bita5e3304dad364fec90eaf5d146f5dd36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
